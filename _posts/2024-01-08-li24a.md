---
title: Efficiently Disentangle Causal Representations
openreview: p8WpFhcKsK
abstract: This paper proposes an efficient approach to learning disentangled representations
  with causal mechanisms based on the difference of conditional probabilities in original
  and new distributions. We approximate the difference with models’ generalization
  abilities so that it fits in the standard machine learning framework and can be
  computed efficiently. In contrast to the state-of-the-art approach, which relies
  on the learner’s adaptation speed to new distribution, the proposed approach only
  requires evaluating the model’s generalization ability. We provide a theoretical
  explanation for the advantage of the proposed method, and our experiments show that
  the proposed technique is 1.9–11.0$\times$ more sample efficient and 9.4–32.4$\times$
  quicker than the previous method on various tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24a
month: 0
tex_title: Efficiently Disentangle Causal Representations
firstpage: 54
lastpage: 71
page: 54-71
order: 54
cycles: false
bibtex_author: Li, Yuanpeng and Hestness, Joel and Elhoseiny, Mohamed and Zhao, Liang
  and Church, Kenneth
author:
- given: Yuanpeng
  family: Li
- given: Joel
  family: Hestness
- given: Mohamed
  family: Elhoseiny
- given: Liang
  family: Zhao
- given: Kenneth
  family: Church
date: 2024-01-08
address:
container-title: Conference on Parsimony and Learning
volume: '234'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 1
  - 8
pdf: https://proceedings.mlr.press/v234/li24a/li24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
