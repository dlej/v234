---
title: Investigating the Catastrophic Forgetting in Multimodal Large Language Model
  Fine-Tuning
openreview: g7rMSiNtmA
abstract: 'Following the success of GPT4, there has been a surge in interest in multimodal
  large language model (MLLM) research. This line of research focuses on developing
  general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However,
  catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails
  to retain similar performance compared to the pre-trained model, still remains an
  inherited problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating
  MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each
  MLLM as an image classifier. We first apply EMT to evaluate several open-source
  fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain
  the same performance levels as their vision encoders on standard image classification
  tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess
  performance throughout the fine-tuning. Interestingly, our results suggest that
  early-stage fine-tuning on an image dataset improves performance across other image
  datasets, by enhancing the alignment of text and language features. However, as
  fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant
  loss of generalizability, even when the image encoder remains frozen. Our results
  suggest that MLLMs have yet to demonstrate performance on par with their vision
  models on standard image classification tasks and the current MLLM fine-tuning procedure
  still has room for improvement.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhai24a
month: 0
tex_title: Investigating the Catastrophic Forgetting in Multimodal Large Language
  Model Fine-Tuning
firstpage: 202
lastpage: 227
page: 202-227
order: 202
cycles: false
bibtex_author: Zhai, Yuexiang and Tong, Shengbang and Li, Xiao and Cai, Mu and Qu,
  Qing and Lee, Yong Jae and Ma, Yi
author:
- given: Yuexiang
  family: Zhai
- given: Shengbang
  family: Tong
- given: Xiao
  family: Li
- given: Mu
  family: Cai
- given: Qing
  family: Qu
- given: Yong Jae
  family: Lee
- given: Yi
  family: Ma
date: 2024-01-08
address:
container-title: Conference on Parsimony and Learning
volume: '234'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 1
  - 8
pdf: https://proceedings.mlr.press/v234/zhai24a/zhai24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
