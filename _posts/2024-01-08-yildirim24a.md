---
title: 'Continual Learning with Dynamic Sparse Training: Exploring Algorithms for
  Effective Model Updates'
openreview: kkz4BbquBy
abstract: Continual learning (CL) refers to the ability of an intelligent system to
  sequentially acquire and retain knowledge from a stream of data with as little computational
  overhead as possible. To this end; regularization, replay, architecture, and parameter
  isolation approaches were introduced to the literature. Parameter isolation using
  a sparse network which enables to allocate distinct parts of the neural network
  to different tasks and also allows to share of parameters between tasks if they
  are similar. Dynamic Sparse Training (DST) is a prominent way to find these sparse
  networks and isolate them for each task. This paper is the first empirical study
  investigating the effect of different DST components under the CL paradigm to fill
  a critical research gap and shed light on the optimal configuration of DST for CL
  if it exists. Therefore, we perform a comprehensive study in which we investigate
  various DST components to find the best topology per task on well-known CIFAR100
  and miniImageNet benchmarks in a task-incremental CL setup since our primary focus
  is to evaluate the performance of various DST criteria, rather than the process
  of mask selection. We found that, at a low sparsity level, Erdos-Renyi Kernel (ERK)
  initialization utilizes the backbone more efficiently and allows to effectively
  learn increments of tasks. At a high sparsity level, however, uniform initialization
  demonstrates more reliable and robust performance. In terms of growth strategy;
  performance is dependent on the defined initialization strategy and the extent of
  sparsity. Finally, adaptivity within DST components is a promising way for better
  continual learners.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yildirim24a
month: 0
tex_title: 'Continual Learning with Dynamic Sparse Training: Exploring Algorithms
  for Effective Model Updates'
firstpage: 94
lastpage: 107
page: 94-107
order: 94
cycles: false
bibtex_author: Yildirim, Murat Onur and Gok, Elif Ceren and Sokar, Ghada and Mocanu,
  Decebal Constantin and Vanschoren, Joaquin
author:
- given: Murat Onur
  family: Yildirim
- given: Elif Ceren
  family: Gok
- given: Ghada
  family: Sokar
- given: Decebal Constantin
  family: Mocanu
- given: Joaquin
  family: Vanschoren
date: 2024-01-08
address:
container-title: Conference on Parsimony and Learning
volume: '234'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 1
  - 8
pdf: https://proceedings.mlr.press/v234/yildirim24a/yildirim24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
