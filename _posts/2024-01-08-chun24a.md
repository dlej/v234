---
title: Sparse Activations with Correlated Weights in Cortex-Inspired Neural Networks
openreview: cyMsUO5J7U
abstract: Although sparse activations are commonly seen in cortical brain circuits,
  the computational benefits of sparse activations are not well understood for machine
  learning. Recent neural network Gaussian Process models have incorporated sparsity
  in infinitely-wide neural network architectures, but these models result in Gram
  matrices that approach the identity matrix with increasing sparsity.  This collapse
  of input pattern similarities in the network representation is due to the use of
  independent weight vectors in the models. In this work, we show how weak correlations
  in the weights can counter this effect.  Correlations in the synaptic weights are
  introduced using a convolutional model, similar to the neural structure of lateral
  connections in the cortex. We show how to theoretically compute the properties of
  infinitely-wide networks with sparse, correlated weights and with rectified linear
  outputs. In particular, we demonstrate how the generalization performance of these
  sparse networks improves by introducing these correlations.  We also show how to
  compute the optimal degree of correlations that result in the best-performing deep
  networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chun24a
month: 0
tex_title: Sparse Activations with Correlated Weights in Cortex-Inspired Neural Networks
firstpage: 248
lastpage: 268
page: 248-268
order: 248
cycles: false
bibtex_author: Chun, Chanwoo and Lee, Daniel
author:
- given: Chanwoo
  family: Chun
- given: Daniel
  family: Lee
date: 2024-01-08
address:
container-title: Conference on Parsimony and Learning
volume: '234'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 1
  - 8
pdf: https://proceedings.mlr.press/v234/chun24a/chun24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
