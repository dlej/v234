---
title: Deep Leakage from Model in Federated Learning
openreview: S8MPHInGqj
abstract: Federated Learning (FL) was conceived as a secure form of distributed learning
  by keeping private training data local and only communicating public model gradients
  between clients. However, a slew of gradient leakage attacks proposed to date undermine
  this claim by proving its insecurity. A common limitation of these attacks is the
  necessity for extensive auxiliary information, such as model weights, optimizers,
  and certain hyperparameters (e.g., learning rate), which are challenging to acquire
  in practical scenarios. Furthermore, several existing algorithms, including FedAvg,
  circumvent the transmission of model gradients in FL by instead sending model weights,
  but the potential security breaches of this approach are seldom considered. In this
  paper, we propose two innovative frameworks, DLM and DLM+, that reveal the potential
  leakage of private local data of clients when transmitting model weights under the
  FL framework. We also conduct a series of experiments to elucidate the impact and
  universality of our attack frameworks. Additionally, we propose and evaluate two
  defenses against the proposed attacks, assessing their protective efficacy.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao24b
month: 0
tex_title: Deep Leakage from Model in Federated Learning
firstpage: 324
lastpage: 340
page: 324-340
order: 324
cycles: false
bibtex_author: Zhao, Zihao and Luo, Mengen and Ding, Wenbo
author:
- given: Zihao
  family: Zhao
- given: Mengen
  family: Luo
- given: Wenbo
  family: Ding
date: 2024-01-08
address:
container-title: Conference on Parsimony and Learning
volume: '234'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 1
  - 8
pdf: https://proceedings.mlr.press/v234/zhao24b/zhao24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
