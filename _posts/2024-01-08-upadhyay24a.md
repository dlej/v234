---
title: Less is More – Towards parsimonious multi-task models using structured sparsity
openreview: 0VU6Vlh0zy
abstract: 'Model sparsification in deep learning promotes simpler, more interpretable
  models with fewer parameters.  This not only reduces the model’s memory footprint
  and computational needs but also shortens inference time. This work focuses on creating
  sparse models optimized for multiple tasks with fewer parameters.  These parsimonious
  models also possess the potential to match or outperform dense models in terms of
  performance. In this work, we introduce channel-wise $l_1/l_2$ group sparsity in
  the shared convolutional layers parameters (or weights) of the multi-task learning
  model. This approach facilitates the removal of extraneous groups i.e., channels
  (due to $l_1$ regularization) and also imposes a penalty on the weights, further
  enhancing the learning efficiency for all tasks (due to $l_2$ regularization). We
  analyzed the results of group sparsity in both single-task and multi-task settings
  on two widely-used multi-task learning datasets: NYU-v2 and CelebAMask-HQ. On both
  datasets, which consist of three different computer vision tasks each, multi-task
  models with approximately 70% sparsity outperform their dense equivalents. We also
  investigate how changing the degree of sparsification influences the model’s performance,
  the overall sparsity percentage, the patterns of sparsity, and the inference time.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: upadhyay24a
month: 0
tex_title: Less is More – Towards parsimonious multi-task models using structured
  sparsity
firstpage: 590
lastpage: 601
page: 590-601
order: 590
cycles: false
bibtex_author: Upadhyay, Richa and Phlypo, Ronald and Saini, Rajkumar and Liwicki,
  Marcus
author:
- given: Richa
  family: Upadhyay
- given: Ronald
  family: Phlypo
- given: Rajkumar
  family: Saini
- given: Marcus
  family: Liwicki
date: 2024-01-08
address:
container-title: Conference on Parsimony and Learning
volume: '234'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 1
  - 8
pdf: https://proceedings.mlr.press/v234/upadhyay24a/upadhyay24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
